{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot 编码\n",
    "\n",
    "one-hot 编码用来把一个类别标签转化为概率分布\n",
    "\n",
    "如果是共有 5 个类别的话\n",
    "\n",
    "例如标签 标签 tensor([1])\n",
    "\n",
    "```python\n",
    "转化为 tensor([\n",
    "    [0, 1, 0, 0, 0]\n",
    "])  # 第 1 号类别的概率为 1，其余为 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "vec = torch.arange(3)\n",
    "F.one_hot(vec, num_classes=5)  # 类别的个数为 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 7])\n",
      "torch.Size([3, 2, 7])\n"
     ]
    }
   ],
   "source": [
    "mat = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])\n",
    "\n",
    "# batch_size, num_steps,  num_classes\n",
    "print(F.one_hot(mat, 7).shape)\n",
    "\n",
    "# num_steps, batch_size, num_classes\n",
    "# 每次拿到所有样本的同一个时间步上的数据集\n",
    "print(F.one_hot(mat.T, 7).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 3])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot 编码逆向转换\n",
    "\n",
    "torch.argmax(torch.tensor([\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 0, 1],\n",
    "]), dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建虚拟数据序列\n",
    "\n",
    "虚拟的数据集的词元是数值 0-19\n",
    "长度为 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "seq = []\n",
    "\n",
    "for i in range(1000 // 10):\n",
    "    # 每次生成长度为 10 的连续序列\n",
    "    start = randint(0, 10)  # [0, 10] 闭区间随机一个数值\n",
    "    for j in range(start, start+10):\n",
    "        seq.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 0, 19)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq), min(seq), max(seq)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = len(seq)  # 序列数据 seq 的长度\n",
    "num_steps = 20  # 时间步长\n",
    "num_classes = 20  # 有多少个种类的词元;用于生成 ont-hot 编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.int64\n",
    "\n",
    "x = torch.tensor(seq, dtype=dtype)\n",
    "X = torch.zeros((T-num_steps, num_steps), dtype=dtype)\n",
    "Y = torch.zeros((T-num_steps, num_steps), dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_steps):\n",
    "    X[:, i] = x[i:T-num_steps+i]\n",
    "    Y[:, i] = x[i+1:T-num_steps+i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  0])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0, :11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6,  7,  8,  9, 10, 11, 12, 13, 14,  0,  1])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[0, :11]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建数据迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50  # 批量大小\n",
    "\n",
    "sample_epochs = X.shape[0] // batch_size\n",
    "num_samples = sample_epochs * batch_size\n",
    "\n",
    "from torch.utils import data\n",
    "dataset = data.TensorDataset(X[:num_samples, :], Y[:num_samples, :])\n",
    "diter =data.DataLoader(dataset=dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 20])  and  torch.Size([50, 20])\n",
      "torch.Size([50, 20])  and  torch.Size([50, 20])\n",
      "torch.Size([50, 20])  and  torch.Size([50, 20])\n",
      "torch.Size([50, 20])  and  torch.Size([50, 20])\n",
      "torch.Size([50, 20])  and  torch.Size([50, 20])\n",
      "torch.Size([50, 20])  and  torch.Size([50, 20])\n",
      "torch.Size([50, 20])  and  torch.Size([50, 20])\n",
      "torch.Size([50, 20])  and  torch.Size([50, 20])\n",
      "torch.Size([50, 20])  and  torch.Size([50, 20])\n",
      "torch.Size([50, 20])  and  torch.Size([50, 20])\n",
      "torch.Size([50, 20])  and  torch.Size([50, 20])\n",
      "torch.Size([50, 20])  and  torch.Size([50, 20])\n",
      "torch.Size([50, 20])  and  torch.Size([50, 20])\n",
      "torch.Size([50, 20])  and  torch.Size([50, 20])\n",
      "torch.Size([50, 20])  and  torch.Size([50, 20])\n",
      "torch.Size([50, 20])  and  torch.Size([50, 20])\n",
      "torch.Size([50, 20])  and  torch.Size([50, 20])\n",
      "torch.Size([50, 20])  and  torch.Size([50, 20])\n",
      "torch.Size([50, 20])  and  torch.Size([50, 20])\n"
     ]
    }
   ],
   "source": [
    "for _X, _Y in diter:\n",
    "    # one_hot_X = F.one_hot(_X, num_classes)\n",
    "    # print(one_hot_X.shape)\n",
    "    print(_X.shape, ' and ', _Y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 浅尝 RNN 层的前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 256  # rnn 隐藏层的输出个数 - 也是隐藏单元个数\n",
    "num_classes = 20  # rnn 隐藏层的输入个数 - 也是 ont-hot 编码的长度 - 代表词元种类个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入为 num_classes\n",
    "# 输出为 num_hiddens\n",
    "# 1 层 rnn 堆叠\n",
    "rnn_layer = nn.RNN(num_classes, num_hiddens, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建隐状态\n",
    "\n",
    "每一个样本的前向传播过程都会产生一个隐状态，因此 $H$ 要有 batch_size 行，每一行都是前向传播过程中产生的一个隐状态, 而每个隐状态其实只是中间层的输出，所以有 num_hiddens 列\n",
    "\n",
    "通过 rnn 层输出的是 new_H 也就是 new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 256])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size = (隐藏层个数，批量大小，神经元数量)\n",
    "# 在下方的测试中仅含有 1 个数据样本\n",
    "state = torch.zeros(size=(1, 1, num_hiddens))\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 20])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (num_steps, batch_size, num_classes)\n",
    "\n",
    "# 创建一个 x_t 样本用于测试 rnn_layer 层的前向传播\n",
    "# 样本数量为 1 序列长度为 3\n",
    "x_t = F.one_hot(torch.tensor([\n",
    "    [1],\n",
    "    [3],\n",
    "    [4]\n",
    "]), 20).float()\n",
    "x_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 256]), torch.Size([1, 1, 256]))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred 是每一个输入的 词元 对应的输出值 形状与 x_t 相同\n",
    "# new_state 是每句话中最后一个词元在每层 rnn 中对应的输出值\n",
    "pred, new_state = rnn_layer(x_t, state)\n",
    "pred.shape, new_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0989,  0.1009, -0.0072, -0.1038,  0.0413,  0.1090,  0.0167],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第 0 号样本在第 3 号个词元对应的输出值的前 7 个值\n",
    "# pred 是每个词元扔进去后在 rnn 层产生的输出，这样就不需要手动来把每个词元丢进去了\n",
    "pred[2][0][:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0989,  0.1009, -0.0072], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 顺序取索引\n",
    "# 第 1 号层的第 0 号样本的最后一个词元的输出值的前 7 个值\n",
    "# new_state[1][0][:3]\n",
    "new_state[0][0][:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 矩阵的展开逻辑\n",
    "\n",
    "从内层的维度向外层逐层展开\n",
    "\n",
    "时间步数，批量大小，one-hot长度\n",
    "\n",
    "第 0 步中的 第 0 号样本 的 one-hot 编码\n",
    "\n",
    "第 0 步中的 第 1 号样本 的 one-hot 编码\n",
    "\n",
    "...\n",
    "\n",
    "以时间步作为分组排列在二维矩阵上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4],\n",
       "        [2, 5],\n",
       "        [3, 6]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 时间步数，批量大小\n",
    "data = torch.tensor([\n",
    "    [1, 4],\n",
    "    [2, 5],\n",
    "    [3, 6]\n",
    "])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0]],\n",
       "\n",
       "        [[0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0]],\n",
       "\n",
       "        [[0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1]]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = F.one_hot(data, 7)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reshape(-1, data.shape[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建 RNN 网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # one-hot 编码长度 就是 20\n",
    "        # 隐藏单元个数\n",
    "        # 几层 rnn 堆叠\n",
    "        self.rnn = nn.RNN(20, 256, 1)\n",
    "\n",
    "        # 使用线性层将其转化为概率分布输出\n",
    "        self.linear = nn.Linear(256, 20)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), 20)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        output = self.linear(Y.reshape(-1, Y.shape[-1]))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, batch_size):\n",
    "        # rnn 网络堆叠层数，批量大小，隐藏单元个数\n",
    "        return torch.zeros(size=(1, batch_size, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RNNModel()\n",
    "state = net.begin_state(50)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "updater = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "for i in range(50):\n",
    "    for X, Y in diter:\n",
    "        y = Y.T.reshape(-1)\n",
    "        # 因为 state 连接了之前的小批量数据 X 和 Y 前向运算的计算图\n",
    "        # 所以在进行新的一轮前向传播计算损失 然后 计算梯度的时候\n",
    "        # 要丢弃之前产生的 计算图，因此要使用 state.detach_() 来实现\n",
    "        # detach_ 是一个 inplace 方法\n",
    "        state = net.begin_state(batch_size=batch_size)\n",
    "\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        updater.zero_grad()\n",
    "        l.backward()\n",
    "        updater.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prefix, num_preds, net, device):  #@save\n",
    "    \"\"\"\n",
    "    在prefix后面生成新字符\n",
    "    num_preds: 后续要生成多少个 词（本节的次元是 字符）\n",
    "    \"\"\"\n",
    "    # batch_size = 1 ==> 对一个字符串做预测\n",
    "    state = net.begin_state(batch_size=1)\n",
    "    outputs = [int(prefix[0])]  # 把 prefix[0] 对应的 数值映射 放进 outputs 中\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    \n",
    "    for y in prefix[1:]:  # 预热期 ==> 把 prefix 中的信息初始化到 state 中\n",
    "        # y 是从 prefix 的 第1号 元素开始取值的\n",
    "        # 当 y 取 prefix[1] 的时候，此时 get_input() 得到的输入是 prefix[0] \n",
    "        # 刚好是错开的，是没有问题的\n",
    "        # 之所以 state 也是网络的输入参数\n",
    "        # 就是使用 state 保存了 prefix 的信息，而 W_hh 是帮助生成 state 用的\n",
    "        inputs = get_input()\n",
    "        _, state = net(inputs, state)\n",
    "        outputs.append(int(y))\n",
    "\n",
    "    # 从这儿开始是预测 prefix 后边的字符\n",
    "    # 每次都用上一次输出的值作为下一次输入的值\n",
    "    # 上一次的输出值是一个时间步上的值，而不是一个序列\n",
    "    # 这刚好和在训练模型时的思路是一致的，使用了时间序列中的\n",
    "    # 某一个时间步上的值的 one-hot 编码作为输入\n",
    "    # 因此这里的 RNN 在真正预测的核心部分，它的输入依旧是时间序列中的\n",
    "    # 某个时间步的值，但用 ont-hot 值来表达\n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "\n",
    "    # 把 index 的值 转化成其 对应的 词（这里是字符）\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('234', 5, net, torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:13:39) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86bb26bcf3701c754d5fd40db657c5b7e15a074c1099b32f8fc32052c18ad091"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
