{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import d2l.torch as d2l\n",
    "\n",
    "seq = []\n",
    "\n",
    "for i in range(1000 // 5):\n",
    "    # 每次生成长度为 10 的连续序列\n",
    "    start = randint(0, 10)  # [0, 10] 闭区间随机一个数值\n",
    "    for j in range(start, start+5):\n",
    "        seq.append(j)\n",
    "\n",
    "T = len(seq)  # 序列数据 seq 的长度\n",
    "num_steps = 20  # 时间步长\n",
    "num_classes = 20  # 有多少个种类的词元;用于生成 ont-hot 编码\n",
    "\n",
    "dtype = torch.int64\n",
    "\n",
    "x = torch.tensor(seq, dtype=dtype)\n",
    "X = torch.zeros((T-num_steps, num_steps), dtype=dtype)\n",
    "Y = torch.zeros((T-num_steps, num_steps), dtype=dtype)\n",
    "\n",
    "for i in range(num_steps):\n",
    "    X[:, i] = x[i:T-num_steps+i]\n",
    "    Y[:, i] = x[i+1:T-num_steps+i+1]\n",
    "\n",
    "batch_size = 50  # 批量大小\n",
    "\n",
    "sample_epochs = X.shape[0] // batch_size\n",
    "num_samples = sample_epochs * batch_size\n",
    "\n",
    "from torch.utils import data\n",
    "dataset = data.TensorDataset(X[:num_samples, :], Y[:num_samples, :])\n",
    "data_iter =data.DataLoader(dataset=dataset, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 官方模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 256])"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens = 256\n",
    "num_classes = 20\n",
    "bacth_size = 50\n",
    "num_steps = 20\n",
    "\n",
    "# len(vocab) 是 RNN 的输入输出的大小 也是 序列的长度 时间步数\n",
    "# num_hiddens 是 RNN 这一层的 单元个数\n",
    "rnn_layer = nn.RNN(num_classes, num_hiddens)\n",
    "\n",
    "# 我们使用张量来初始化隐状态，它的形状是（隐藏层数，批量大小，隐藏单元数）\n",
    "state = torch.zeros((1, batch_size, num_hiddens))\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 50, 256]), torch.Size([1, 50, 256]))"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(size=(num_steps, batch_size, num_classes))\n",
    "Y, state_new = rnn_layer(X, state)\n",
    "# Y （时间步数，批量，隐藏层输出=单元数）\n",
    "Y.shape, state_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class RNNModel(nn.Module):\n",
    "    \"\"\"循环神经网络模型\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_hiddens = self.rnn.hidden_size\n",
    "        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
    "        else:\n",
    "            self.num_directions = 2\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)\n",
    "        Y, state = self.rnn(X, state)\n",
    "        # 全连接层首先将Y的形状改为(时间步数*批量大小,隐藏单元数)\n",
    "        # 它的输出形状是(时间步数*批量大小,词表大小)。\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # nn.GRU以张量作为隐状态\n",
    "            return  torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens),\n",
    "                                device=device)\n",
    "        else:\n",
    "            # nn.LSTM以元组作为隐状态\n",
    "            return (torch.zeros((\n",
    "                self.num_directions * self.rnn.num_layers,\n",
    "                batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((\n",
    "                        self.num_directions * self.rnn.num_layers,\n",
    "                        batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练和预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = d2l.try_gpu()\n",
    "net = RNNModel(rnn_layer, vocab_size=num_classes)\n",
    "net = net.to(device)\n",
    "\n",
    "num_epochs, lr = 500, 1\n",
    "updater = torch.optim.SGD(net.parameters(), lr=0.05)\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net, train_iter, loss, updater, device, use_random_iter\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     ppl, speed = d2l.train_epoch_ch8(net, data_iter, loss, updater, device, False)\n",
    "#     print(ppl, ' and ', speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "state = net.begin_state(batch_size=batch_size, device=device)\n",
    "\n",
    "for i in range(2):\n",
    "    for X, Y in data_iter:\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # 这个 state 为啥会有梯度，我也是没法理解啊\n",
    "        print(state.requires_grad)\n",
    "        state.detach_()\n",
    "        \n",
    "        # 通过这个方式也可以\n",
    "        # 所以说果然是 state 造成的问题\n",
    "        # state = net.begin_state(batch_size=batch_size, device=device)\n",
    "\n",
    "\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        # print(l)\n",
    "        updater.zero_grad()\n",
    "        l.backward()\n",
    "        updater.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ch8(prefix, num_preds, net, device):  #@save\n",
    "    \"\"\"\n",
    "    在prefix后面生成新字符\n",
    "    num_preds: 后续要生成多少个 词（本节的次元是 字符）\n",
    "    \"\"\"\n",
    "    # batch_size = 1 ==> 对一个字符串做预测\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [int(prefix[0])]  # 把 prefix[0] 对应的 数值映射 放进 outputs 中\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    \n",
    "    for y in prefix[1:]:  # 预热期 ==> 把 prefix 中的信息初始化到 state 中\n",
    "        # y 是从 prefix 的 第1号 元素开始取值的\n",
    "        # 当 y 取 prefix[1] 的时候，此时 get_input() 得到的输入是 prefix[0] \n",
    "        # 刚好是错开的，是没有问题的\n",
    "        # 之所以 state 也是网络的输入参数\n",
    "        # 就是使用 state 保存了 prefix 的信息，而 W_hh 是帮助生成 state 用的\n",
    "        inputs = get_input()\n",
    "        _, state = net(inputs, state)\n",
    "        outputs.append(int(y))\n",
    "\n",
    "    # 从这儿开始是预测 prefix 后边的字符\n",
    "    # 每次都用上一次输出的值作为下一次输入的值\n",
    "    # 上一次的输出值是一个时间步上的值，而不是一个序列\n",
    "    # 这刚好和在训练模型时的思路是一致的，使用了时间序列中的\n",
    "    # 某一个时间步上的值的 one-hot 编码作为输入\n",
    "    # 因此这里的 RNN 在真正预测的核心部分，它的输入依旧是时间序列中的\n",
    "    # 某个时间步的值，但用 ont-hot 值来表达\n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "\n",
    "    # 把 index 的值 转化成其 对应的 词（这里是字符）\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ch8('123', 3, net, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 5, 6, 6, 5, 6, 6, 5]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ch8('123', 10, net, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n对于 RNN 中的 state 也是一样的\\n在同一批量中即使每次循环生成每个时间步的 state 时，每次都会和 权重参数建立联系\\n但是在同一个批量中进行这个过程是在完成前向传播且建立计算图的过程\\n这个过程中并没有进行反向传播，所以计算图就一直存在\\n\\n而当完成一个批量之后，上一个批量的最终 state 在下一个批量中参与运算\\n而上一个批量的 state 所关联的计算图已经被销毁了\\n所以下一个批量若进行反向传播的话，就会报错了\\n所以要使用 detach_ 来完成与上一个批量计算中的计算图分离的操作才可以\\n\\n而在 RNN 中使用 backward(retain_graph=True) 会产生新的问题\\n在每一个带有梯度变量的 tensor 中记录了 version\\n若 version 不为 1 ，也是不能完成梯度计算的??\\n'"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.tensor([1], dtype=torch.float32, requires_grad=True)\n",
    "x2 = torch.tensor([1], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "state = torch.tensor([4], dtype=torch.float32)\n",
    "\n",
    "y1 = 2 * x1 + state * x2\n",
    "# 如果在 y1 执行 backward 的时候不保存计算图 retain_graph=True\n",
    "# 那么在执行 y2.backward() 的时候，由于 y1 进行过反向传播了\n",
    "# 所以计算图就已经被自动销毁，丢失了\n",
    "# 因此就不能完成反向传播的任务了\n",
    "# 只有当再一次执行了前向传播，才会再次建立计算图，这时可以反向传播\n",
    "y1.backward(retain_graph=True)\n",
    "x1.grad.zero_(), x2.grad.zero_()\n",
    "y2 = 2 * x1 + y1 * x2\n",
    "y2.backward()\n",
    "\n",
    "\"\"\"\n",
    "对于 RNN 中的 state 也是一样的\n",
    "在同一批量中即使每次循环生成每个时间步的 state 时，每次都会和 权重参数建立联系\n",
    "但是在同一个批量中进行这个过程是在完成前向传播且建立计算图的过程\n",
    "这个过程中并没有进行反向传播，所以计算图就一直存在\n",
    "\n",
    "而当完成一个批量之后，上一个批量的最终 state 在下一个批量中参与运算\n",
    "而上一个批量的 state 所关联的计算图已经被销毁了\n",
    "所以下一个批量若进行反向传播的话，就会报错了\n",
    "所以要使用 detach_ 来完成与上一个批量计算中的计算图分离的操作才可以\n",
    "\n",
    "而在 RNN 中使用 backward(retain_graph=True) 会产生新的问题\n",
    "在每一个带有梯度变量的 tensor 中记录了 version\n",
    "若 version 不为 1 ，也是不能完成梯度计算的??\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4.]), tensor([10.]))"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.grad, x2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, True)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.tensor([1], dtype=torch.float32, requires_grad=True)\n",
    "x2 = torch.tensor([1], dtype=torch.float32, requires_grad=True)\n",
    "state = torch.tensor([4], dtype=torch.float32)\n",
    "x1._version, x2._version, x1.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = 2 * x1 + state * x2\n",
    "# 如果在 y1 执行 backward 的时候不保存计算图 retain_graph=True\n",
    "# 那么在执行 y2.backward() 的时候，由于 y1 进行过反向传播了\n",
    "# 所以计算图就已经被自动销毁，丢失了\n",
    "# 因此就不能完成反向传播的任务了\n",
    "# 只有当再一次执行了前向传播，才会再次建立计算图，这时可以反向传播\n",
    "y1.backward(retain_graph=True)\n",
    "x1.grad.zero_(), x2.grad.zero_()\n",
    "y2 = 2 * x1 + y1 * x2\n",
    "y2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1._version, x2._version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.detach_()\n",
    "x2.detach_()\n",
    "\n",
    "x1 += 1\n",
    "x2 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1._version, x2._version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.], requires_grad=True)"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.], requires_grad=True)"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.detach_()\n",
    "x2.detach_()\n",
    "\n",
    "x1 += 1\n",
    "x2 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1._version, x2._version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, True)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.tensor([1], dtype=torch.float32, requires_grad=True)\n",
    "x2 = torch.tensor([1], dtype=torch.float32, requires_grad=True)\n",
    "state = torch.tensor([4], dtype=torch.float32)\n",
    "x1._version, x2._version, x1.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = 2 * x1 + state * x2\n",
    "y2 = 2 * x1 + y1 * x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.requires_grad, y2.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.requires_grad  # 为什么 y1 会附有梯度？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:13:39) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86bb26bcf3701c754d5fd40db657c5b7e15a074c1099b32f8fc32052c18ad091"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
